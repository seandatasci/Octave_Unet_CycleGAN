{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "octave_unet_cyclegan.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_xnMOsbqHz61"
      },
      "source": [
        "# CycleGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ITZuApL56Mny"
      },
      "source": [
        "\n",
        "This notebook assumes you are familiar with Pix2Pix, which you can learn about in the [Pix2Pix tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix). The code for CycleGAN is similar, the main difference is an additional loss function, and the use of unpaired training data.\n",
        "\n",
        "CycleGAN uses a cycle consistency loss to enable training without the need for paired data. In other words, it can translate from one domain to another without a one-to-one mapping between the source and target domain. \n",
        "\n",
        "This opens up the possibility to do a lot of interesting tasks like photo-enhancement, image colorization, style transfer, etc. All you need is the source and the target dataset (which is simply a directory of images).\n",
        "\n",
        "![Output Image 1](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/horse2zebra_1.png?raw=1)\n",
        "![Output Image 2](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/horse2zebra_2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "## Set up the input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fGHWOKPX4ta"
      },
      "source": [
        "Install the [tensorflow_examples](https://github.com/tensorflow/examples) package that enables importing of the generator and the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bJ1ROiQxJ-vY",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/tensorflow/examples.git\n",
        "!pip install keras-rectified-adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lhSsUx9Nyb3t",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YfIk2es3hJEd",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "## Input Pipeline\n",
        "\n",
        "This tutorial trains a model to translate from images of horses, to images of zebras. You can find this dataset and similar ones [here](https://www.tensorflow.org/datasets/datasets#cycle_gan). \n",
        "\n",
        "As mentioned in the [paper](https://arxiv.org/abs/1703.10593), apply random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting.\n",
        "\n",
        "This is similar to what was done in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#load_the_dataset)\n",
        "\n",
        "* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`.\n",
        "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iuGVPOo7Cce0",
        "colab": {}
      },
      "source": [
        "dataset, metadata = tfds.load('cycle_gan/horse2zebra',\n",
        "                              with_info=True, as_supervised=True)\n",
        "\n",
        "train_horses, train_zebras = dataset['trainA'], dataset['trainB']\n",
        "test_horses, test_zebras = dataset['testA'], dataset['testB']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2CbTEt448b4R",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 1\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yn3IwqhiIszt",
        "colab": {}
      },
      "source": [
        "def random_crop(image):\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "muhR2cgbLKWW",
        "colab": {}
      },
      "source": [
        "# normalizing the images to [-1, 1]\n",
        "def normalize(image):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = (image / 127.5) - 1\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fVQOjcPVLrUc",
        "colab": {}
      },
      "source": [
        "def random_jitter(image):\n",
        "  # resizing to 286 x 286 x 3\n",
        "  image = tf.image.resize(image, [286, 286],\n",
        "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  # randomly cropping to 256 x 256 x 3\n",
        "  image = random_crop(image)\n",
        "\n",
        "  # random mirroring\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tyaP4hLJ8b4W",
        "colab": {}
      },
      "source": [
        "def preprocess_image_train(image, label):\n",
        "  image = random_jitter(image)\n",
        "  image = normalize(image)\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VB3Z6D_zKSru",
        "colab": {}
      },
      "source": [
        "def preprocess_image_test(image, label):\n",
        "  image = normalize(image)\n",
        "  return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RsajGXxd5JkZ",
        "colab": {}
      },
      "source": [
        "train_horses = train_horses.map(\n",
        "    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(1)\n",
        "\n",
        "train_zebras = train_zebras.map(\n",
        "    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(1)\n",
        "\n",
        "test_horses = test_horses.map(\n",
        "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(1)\n",
        "\n",
        "test_zebras = test_zebras.map(\n",
        "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e3MhJ3zVLPan",
        "colab": {}
      },
      "source": [
        "sample_horse = next(iter(train_horses))\n",
        "sample_zebra = next(iter(train_zebras))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4pOYjMk_KfIB",
        "colab": {}
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Horse')\n",
        "plt.imshow(sample_horse[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Horse with random jitter')\n",
        "plt.imshow(random_jitter(sample_horse[0]) * 0.5 + 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0KJyB9ENLb2y",
        "colab": {}
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Zebra')\n",
        "plt.imshow(sample_zebra[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Zebra with random jitter')\n",
        "plt.imshow(random_jitter(sample_zebra[0]) * 0.5 + 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hvX8sKsfMaio"
      },
      "source": [
        "## Import and reuse the Pix2Pix models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cGrL73uCd-_M"
      },
      "source": [
        "Import the generator and the discriminator used in [Pix2Pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) via the installed [tensorflow_examples](https://github.com/tensorflow/examples) package.\n",
        "\n",
        "The model architecture used in this tutorial is very similar to what was used in [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). Some of the differences are:\n",
        "\n",
        "* Cyclegan uses [instance normalization](https://arxiv.org/abs/1607.08022) instead of [batch normalization](https://arxiv.org/abs/1502.03167).\n",
        "\n",
        "There are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. \n",
        "\n",
        "* Generator `G` learns to transform image `X` to image `Y`. $(G: X -> Y)$\n",
        "* Generator `F` learns to transform image `Y` to image `X`. $(F: Y -> X)$\n",
        "* Discriminator `D_X` learns to differentiate between image `X` and generated image `X` (`F(Y)`).\n",
        "* Discriminator `D_Y` learns to differentiate between image `Y` and generated image `Y` (`G(X)`).\n",
        "\n",
        "![Cyclegan model](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/cyclegan_model.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2WBqHZKXXVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile keras_octave_conv.py\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "__all__ = ['OctaveConv2D', 'octave_conv_2d']\n",
        "\n",
        "\n",
        "class OctaveConv2D(Layer):\n",
        "    \"\"\"Octave convolutions.\n",
        "    # Arguments\n",
        "        octave: The division of the spatial dimensions by a power of 2.\n",
        "        ratio_out: The ratio of filters for lower spatial resolution.\n",
        "    # References\n",
        "        - [Drop an Octave: Reducing Spatial Redundancy in Convolutional Neural Networks with Octave Convolution]\n",
        "          (https://arxiv.org/pdf/1904.05049.pdf)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 filters,\n",
        "                 kernel_size=(3,3),\n",
        "                 octave=2,\n",
        "                 ratio_out=0.125,\n",
        "                 strides=(1, 1),\n",
        "                 data_format=None,\n",
        "                 dilation_rate=(1, 1),\n",
        "                 activation=None,\n",
        "                 use_bias=False,\n",
        "                 use_transpose=False,\n",
        "                 kernel_initializer='he_normal',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(OctaveConv2D, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.octave = octave\n",
        "        self.ratio_out = ratio_out\n",
        "        self.strides = strides\n",
        "        self.data_format = data_format\n",
        "        self.dilation_rate = dilation_rate\n",
        "        self.use_bias = use_bias\n",
        "        self.use_transpose = use_transpose\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.bias_initializer = bias_initializer\n",
        "        self.kernel_regularizer = kernel_regularizer\n",
        "        self.bias_regularizer = bias_regularizer\n",
        "        self.activity_regularizer = activity_regularizer\n",
        "        self.kernel_constraint = kernel_constraint\n",
        "        self.bias_constraint = bias_constraint\n",
        "\n",
        "        self.filters_low = int(filters * self.ratio_out)\n",
        "        self.filters_high = filters - self.filters_low\n",
        "\n",
        "        self.conv_high_to_high, self.conv_low_to_high = None, None\n",
        "        if self.use_transpose:\n",
        "          if self.filters_high > 0:\n",
        "              self.conv_high_to_high = self._init_transconv(self.filters_high, name='{}-Trans-Conv2D-HH'.format(self.name))\n",
        "              self.conv_low_to_high = self._init_transconv(self.filters_high, name='{}-Conv2D-LH'.format(self.name))\n",
        "          self.conv_low_to_low, self.conv_high_to_low = None, None\n",
        "          if self.filters_low > 0:\n",
        "              self.conv_low_to_low = self._init_transconv(self.filters_low, name='{}-Trans-Conv2D-HL'.format(self.name))\n",
        "              self.conv_high_to_low = self._init_transconv(self.filters_low, name='{}-Trans-Conv2D-LL'.format(self.name))\n",
        "          self.pooling = AveragePooling2D(\n",
        "              pool_size=self.octave,\n",
        "              padding='valid',\n",
        "              data_format=data_format,\n",
        "              name='{}-AveragePooling2D'.format(self.name),\n",
        "          )\n",
        "          self.up_sampling = UpSampling2D(\n",
        "              size=self.octave,\n",
        "              data_format=data_format,\n",
        "              name='{}-UpSampling2D'.format(self.name)\n",
        "          )\n",
        "        else:\n",
        "          if self.filters_high > 0:\n",
        "              self.conv_high_to_high = self._init_conv(self.filters_high, name='{}-Conv2D-HH'.format(self.name))\n",
        "              self.conv_low_to_high = self._init_conv(self.filters_high, name='{}-Conv2D-LH'.format(self.name))\n",
        "          self.conv_low_to_low, self.conv_high_to_low = None, None\n",
        "          if self.filters_low > 0:\n",
        "              self.conv_low_to_low = self._init_conv(self.filters_low, name='{}-Conv2D-HL'.format(self.name))\n",
        "              self.conv_high_to_low = self._init_conv(self.filters_low, name='{}-Conv2D-LL'.format(self.name))\n",
        "          self.pooling = AveragePooling2D(\n",
        "              pool_size=self.octave,\n",
        "              padding='valid',\n",
        "              data_format=data_format,\n",
        "              name='{}-AveragePooling2D'.format(self.name),\n",
        "          )\n",
        "          self.up_sampling = UpSampling2D(\n",
        "              size=self.octave,\n",
        "              data_format=data_format,\n",
        "              name='{}-UpSampling2D'.format(self.name)\n",
        "          )\n",
        "    def _init_transconv(self, filters, name):\n",
        "        return Conv2DTranspose(\n",
        "            filters=filters,\n",
        "            kernel_size=self.kernel_size,\n",
        "            strides=self.strides,\n",
        "            padding='same',\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate,\n",
        "            use_bias=self.use_bias,\n",
        "            kernel_initializer=self.kernel_initializer,\n",
        "            bias_initializer=self.bias_initializer,\n",
        "            kernel_regularizer=self.kernel_regularizer,\n",
        "            bias_regularizer=self.bias_regularizer,\n",
        "            activity_regularizer=self.activity_regularizer,\n",
        "            kernel_constraint=self.kernel_constraint,\n",
        "            bias_constraint=self.bias_constraint,\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    def _init_conv(self, filters, name):\n",
        "        return Conv2D(\n",
        "            filters=filters,\n",
        "            kernel_size=self.kernel_size,\n",
        "            strides=self.strides,\n",
        "            padding='same',\n",
        "            data_format=self.data_format,\n",
        "            dilation_rate=self.dilation_rate,\n",
        "            use_bias=self.use_bias,\n",
        "            kernel_initializer=self.kernel_initializer,\n",
        "            bias_initializer=self.bias_initializer,\n",
        "            kernel_regularizer=self.kernel_regularizer,\n",
        "            bias_regularizer=self.bias_regularizer,\n",
        "            activity_regularizer=self.activity_regularizer,\n",
        "            kernel_constraint=self.kernel_constraint,\n",
        "            bias_constraint=self.bias_constraint,\n",
        "            name=name,\n",
        "        )\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if isinstance(input_shape, list):\n",
        "            input_shape_high, input_shape_low = input_shape\n",
        "        else:\n",
        "            input_shape_high, input_shape_low = input_shape, None\n",
        "        if self.data_format == 'channels_first':\n",
        "            channel_axis, rows_axis, cols_axis = 1, 2, 3\n",
        "        else:\n",
        "            rows_axis, cols_axis, channel_axis = 1, 2, 3\n",
        "        if input_shape_high[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the higher spatial inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        if input_shape_low is not None and input_shape_low[channel_axis] is None:\n",
        "            raise ValueError('The channel dimension of the lower spatial inputs '\n",
        "                             'should be defined. Found `None`.')\n",
        "        if input_shape_high[rows_axis] is not None and input_shape_high[rows_axis] % self.octave != 0 or \\\n",
        "           input_shape_high[cols_axis] is not None and input_shape_high[cols_axis] % self.octave != 0:\n",
        "            raise ValueError('The rows and columns of the higher spatial inputs should be divisible by the octave. '\n",
        "                             'Found {} and {}.'.format(input_shape_high, self.octave))\n",
        "        if input_shape_low is None:\n",
        "            self.conv_low_to_high, self.conv_low_to_low = None, None\n",
        "\n",
        "        if self.conv_high_to_high is not None:\n",
        "            with K.name_scope(self.conv_high_to_high.name):\n",
        "                self.conv_high_to_high.build(input_shape_high)\n",
        "        if self.conv_low_to_high is not None:\n",
        "            with K.name_scope(self.conv_low_to_high.name):\n",
        "                self.conv_low_to_high.build(input_shape_low)\n",
        "        if self.conv_high_to_low is not None:\n",
        "            with K.name_scope(self.conv_high_to_low.name):\n",
        "                self.conv_high_to_low.build(input_shape_high)\n",
        "        if self.conv_low_to_low is not None:\n",
        "            with K.name_scope(self.conv_low_to_low.name):\n",
        "                self.conv_low_to_low.build(input_shape_low)\n",
        "        super(OctaveConv2D, self).build(input_shape)\n",
        "\n",
        "    @property\n",
        "    def trainable_weights(self):\n",
        "        weights = []\n",
        "        if self.conv_high_to_high is not None:\n",
        "            weights += self.conv_high_to_high.trainable_weights\n",
        "        if self.conv_low_to_high is not None:\n",
        "            weights += self.conv_low_to_high.trainable_weights\n",
        "        if self.conv_high_to_low is not None:\n",
        "            weights += self.conv_high_to_low.trainable_weights\n",
        "        if self.conv_low_to_low is not None:\n",
        "            weights += self.conv_low_to_low.trainable_weights\n",
        "        return weights\n",
        "\n",
        "    @property\n",
        "    def non_trainable_weights(self):\n",
        "        weights = []\n",
        "        if self.conv_high_to_high is not None:\n",
        "            weights += self.conv_high_to_high.non_trainable_weights\n",
        "        if self.conv_low_to_high is not None:\n",
        "            weights += self.conv_low_to_high.non_trainable_weights\n",
        "        if self.conv_high_to_low is not None:\n",
        "            weights += self.conv_high_to_low.non_trainable_weights\n",
        "        if self.conv_low_to_low is not None:\n",
        "            weights += self.conv_low_to_low.non_trainable_weights\n",
        "        return weights\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if isinstance(input_shape, list):\n",
        "            input_shape_high, input_shape_low = input_shape\n",
        "        else:\n",
        "            input_shape_high, input_shape_low = input_shape, None\n",
        "\n",
        "        output_shape_high = None\n",
        "        if self.filters_high > 0:\n",
        "            output_shape_high = self.conv_high_to_high.compute_output_shape(input_shape_high)\n",
        "        output_shape_low = None\n",
        "        if self.filters_low > 0:\n",
        "            output_shape_low = self.conv_high_to_low.compute_output_shape(\n",
        "                self.pooling.compute_output_shape(input_shape_high),\n",
        "            )\n",
        "\n",
        "        if self.filters_low == 0:\n",
        "            return output_shape_high\n",
        "        if self.filters_high == 0:\n",
        "            return output_shape_low\n",
        "        return [output_shape_high, output_shape_low]\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if isinstance(inputs, list):\n",
        "            inputs_high, inputs_low = inputs\n",
        "        else:\n",
        "            inputs_high, inputs_low = inputs, None\n",
        "\n",
        "        outputs_high_to_high, outputs_low_to_high = 0.0, 0.0\n",
        "        if self.use_transpose:\n",
        "          if self.conv_high_to_high is not None:\n",
        "              outputs_high_to_high = self.conv_high_to_high(inputs_high)\n",
        "          if self.conv_low_to_high is not None:\n",
        "              outputs_low_to_high = self.up_sampling(self.conv_low_to_high(inputs_low))\n",
        "          outputs_high = outputs_high_to_high + outputs_low_to_high\n",
        "\n",
        "          outputs_low_to_low, outputs_high_to_low = 0.0, 0.0\n",
        "          if self.conv_low_to_low is not None:\n",
        "              outputs_low_to_low = self.conv_low_to_low(inputs_low)\n",
        "          if self.conv_high_to_low is not None:\n",
        "              outputs_high_to_low = self.pooling(self.conv_high_to_low(inputs_high))\n",
        "          outputs_low = outputs_low_to_low + outputs_high_to_low\n",
        "\n",
        "          if self.filters_low == 0:\n",
        "              return outputs_high\n",
        "          if self.filters_high == 0:\n",
        "              return outputs_low\n",
        "        else:\n",
        "          if self.conv_high_to_high is not None:\n",
        "              outputs_high_to_high = self.conv_high_to_high(inputs_high)\n",
        "          if self.conv_low_to_high is not None:\n",
        "              outputs_low_to_high = self.up_sampling(self.conv_low_to_high(inputs_low))\n",
        "          outputs_high = outputs_high_to_high + outputs_low_to_high\n",
        "\n",
        "          outputs_low_to_low, outputs_high_to_low = 0.0, 0.0\n",
        "          if self.conv_low_to_low is not None:\n",
        "              outputs_low_to_low = self.conv_low_to_low(inputs_low)\n",
        "          if self.conv_high_to_low is not None:\n",
        "              outputs_high_to_low = self.conv_high_to_low(self.pooling(inputs_high))\n",
        "          outputs_low = outputs_low_to_low + outputs_high_to_low\n",
        "\n",
        "          if self.filters_low == 0:\n",
        "              return outputs_high\n",
        "          if self.filters_high == 0:\n",
        "              return outputs_low\n",
        "        return [outputs_high, outputs_low]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'filters': self.filters,\n",
        "            'kernel_size': self.kernel_size,\n",
        "            'octave': self.octave,\n",
        "            'ratio_out': self.ratio_out,\n",
        "            'strides': self.strides,\n",
        "            'data_format': self.data_format,\n",
        "            'dilation_rate': self.dilation_rate,\n",
        "            'use_bias': self.use_bias,\n",
        "            'kernel_initializer': self.kernel_initializer,\n",
        "            'bias_initializer': self.bias_initializer,\n",
        "            'kernel_regularizer': self.kernel_regularizer,\n",
        "            'bias_regularizer': self.bias_regularizer,\n",
        "            'activity_regularizer': self.activity_regularizer,\n",
        "            'kernel_constraint': self.kernel_constraint,\n",
        "            'bias_constraint': self.bias_constraint\n",
        "        }\n",
        "        base_config = super(OctaveConv2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "class instance_norm(tf.keras.layers.Layer):\n",
        "  \"\"\"Instance Normalization Layer (https://arxiv.org/abs/1607.08022).\"\"\"\n",
        "\n",
        "  def __init__(self, epsilon=1e-5):\n",
        "    super(instance_norm, self).__init__()\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.scale = self.add_weight(\n",
        "        name='scale',\n",
        "        shape=input_shape[-1:],\n",
        "        initializer=tf.random_normal_initializer(1., 0.02),\n",
        "        trainable=True)\n",
        "\n",
        "    self.offset = self.add_weight(\n",
        "        name='offset',\n",
        "        shape=input_shape[-1:],\n",
        "        initializer='zeros',\n",
        "        trainable=True)\n",
        "\n",
        "  def call(self, x):\n",
        "    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n",
        "    inv = tf.math.rsqrt(variance + self.epsilon)\n",
        "    normalized = (x - mean) * inv\n",
        "    return self.scale * normalized + self.offset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiMxFnhzXW8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile model.py\n",
        "# model code all in this cell\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from keras_radam.training import RAdamOptimizer\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras_octave_conv import OctaveConv2D\n",
        "from keras_octave_conv import instance_norm\n",
        "\n",
        "      \n",
        "def unet(pretrained_weights = None,input_size = (256,256,3)):\n",
        "    inputs = Input(input_size)\n",
        "    # downsampling for lower\n",
        "    low = layers.AveragePooling2D(2)(inputs)\n",
        "    high1, low1 = OctaveConv2D(64)([inputs,low])\n",
        "    high1 = instance_norm()(high1)\n",
        "    high1 = layers.Activation(\"relu\")(high1)\n",
        "    low1 = instance_norm()(low1)\n",
        "    low1 = layers.Activation(\"relu\")(low1)\n",
        "    high1, low1 = OctaveConv2D(64)([high1, low1])\n",
        "    high1 = instance_norm()(high1)\n",
        "    high1 = layers.Activation(\"relu\")(high1)\n",
        "    low1 = instance_norm()(low1)\n",
        "    low1 = layers.Activation(\"relu\")(low1)\n",
        "    pool1high = layers.MaxPooling2D(2)(high1)\n",
        "    pool1low = layers.MaxPooling2D(2)(low1)\n",
        "    \n",
        "    high2, low2 = OctaveConv2D(128)([pool1high,pool1low])\n",
        "    high2 = instance_norm()(high2)\n",
        "    high2 = layers.Activation(\"relu\")(high2)\n",
        "    low2 = instance_norm()(low2)\n",
        "    low2 = layers.Activation(\"relu\")(low2)\n",
        "    high2, low2 = OctaveConv2D(128)([high2, low2])\n",
        "    high2 = instance_norm()(high2)\n",
        "    high2 = layers.Activation(\"relu\")(high2)\n",
        "    low2 = instance_norm()(low2)\n",
        "    low2 = layers.Activation(\"relu\")(low2)\n",
        "    pool2high = layers.MaxPooling2D(2)(high2)\n",
        "    pool2low = layers.MaxPooling2D(2)(low2)\n",
        "    \n",
        "    high3, low3 = OctaveConv2D(256)([pool2high,pool2low])\n",
        "    high3 = instance_norm()(high3)\n",
        "    high3 = layers.Activation(\"relu\")(high3)\n",
        "    low3 = instance_norm()(low3)\n",
        "    low3 = layers.Activation(\"relu\")(low3)\n",
        "    high3, low3 = OctaveConv2D(256)([high3, low3])\n",
        "    high3 = instance_norm()(high3)\n",
        "    high3 = layers.Activation(\"relu\")(high3)\n",
        "    low3 = instance_norm()(low3)\n",
        "    low3 = layers.Activation(\"relu\")(low3)\n",
        "    pool3high = layers.MaxPooling2D(2)(high3)\n",
        "    pool3low = layers.MaxPooling2D(2)(low3)\n",
        "    \n",
        "    high4, low4 = OctaveConv2D(512)([pool3high,pool3low])\n",
        "    high4 = instance_norm()(high4)\n",
        "    high4 = layers.Activation(\"relu\")(high4)\n",
        "    low4 = instance_norm()(low4)\n",
        "    low4 = layers.Activation(\"relu\")(low4)\n",
        "    high4, low4 = OctaveConv2D(512)([high4, low4])\n",
        "    high4 = instance_norm()(high4)\n",
        "    high4 = layers.Activation(\"relu\")(high4)\n",
        "    low4 = instance_norm()(low4)\n",
        "    low4 = layers.Activation(\"relu\")(low4)\n",
        "    pool4high = layers.MaxPooling2D(2)(high4)\n",
        "    pool4low = layers.MaxPooling2D(2)(low4)\n",
        "\n",
        "    high5, low5 = OctaveConv2D(1024)([pool4high, pool4low])\n",
        "    high5 = instance_norm()(high5)\n",
        "    high5 = layers.Activation(\"relu\")(high5)\n",
        "    low5 = instance_norm()(low5)\n",
        "    low5 = layers.Activation(\"relu\")(low5)\n",
        "    high5 = Dropout(0.4)(high5)\n",
        "    low5 = Dropout(0.4)(low5)\n",
        "    high5, low5 = OctaveConv2D(1024)([high5, low5])\n",
        "    high5 = instance_norm()(high5)\n",
        "    high5 = layers.Activation(\"relu\")(high5)\n",
        "    low5 = instance_norm()(low5)\n",
        "    low5 = layers.Activation(\"relu\")(low5)\n",
        "    high5 = Dropout(0.4)(high5)\n",
        "    low5 = Dropout(0.4)(low5)\n",
        "    \n",
        "    uphigh6, uplow6 = OctaveConv2D(512, use_transpose=True, strides=(2,2))([high5,low5])\n",
        "    uphigh6 = instance_norm()(uphigh6)\n",
        "    uphigh6 = layers.Activation(\"relu\")(uphigh6)\n",
        "    uplow6 = instance_norm()(uplow6)\n",
        "    uplow6 = layers.Activation(\"relu\")(uplow6)\n",
        "    merge6high = concatenate([high4,uphigh6], axis = 3)\n",
        "    merge6low = concatenate([low4,uplow6], axis = 3)\n",
        "    high6, low6 = OctaveConv2D(512)([merge6high,merge6low])\n",
        "    high6 = instance_norm()(high6)\n",
        "    high6 = layers.Activation(\"relu\")(high6)\n",
        "    low6 = instance_norm()(low6)\n",
        "    low6 = layers.Activation(\"relu\")(low6)\n",
        "    high6, low6 = OctaveConv2D(512)([high6, low6])\n",
        "    high6 = instance_norm()(high6)\n",
        "    high6 = layers.Activation(\"relu\")(high6)\n",
        "    low6 = instance_norm()(low6)\n",
        "    low6 = layers.Activation(\"relu\")(low6)\n",
        "\n",
        "\n",
        "    uphigh7, uplow7 = OctaveConv2D(256, use_transpose=True, strides=(2,2))([high6, low6])\n",
        "    uphigh7 = instance_norm()(uphigh7)\n",
        "    uphigh7 = layers.Activation(\"relu\")(uphigh7)\n",
        "    uplow7 = instance_norm()(uplow7)\n",
        "    uplow7 = layers.Activation(\"relu\")(uplow7)\n",
        "    merge7high = concatenate([high3,uphigh7], axis = 3)\n",
        "    merge7low = concatenate([low3,uplow7], axis = 3)\n",
        "    high7, low7 = OctaveConv2D(256)([merge7high, merge7low])\n",
        "    high7 = instance_norm()(high7)\n",
        "    high7 = layers.Activation(\"relu\")(high7)\n",
        "    low7 = instance_norm()(low7)\n",
        "    low7 = layers.Activation(\"relu\")(low7)\n",
        "    high7, low7 = OctaveConv2D(256)([high7, low7])\n",
        "    high7 = instance_norm()(high7)\n",
        "    high7 = layers.Activation(\"relu\")(high7)\n",
        "    low7 = instance_norm()(low7)\n",
        "    low7 = layers.Activation(\"relu\")(low7)\n",
        "\n",
        "    uphigh8, uplow8 = OctaveConv2D(128, use_transpose=True, strides=(2,2))([high7, low7])\n",
        "    uphigh8 = instance_norm()(uphigh8)\n",
        "    uphigh8 = layers.Activation(\"relu\")(uphigh8)\n",
        "    uplow8 = instance_norm()(uplow8)\n",
        "    uplow8 = layers.Activation(\"relu\")(uplow8)\n",
        "    merge8high = concatenate([high2,uphigh8], axis = 3)\n",
        "    merge8low = concatenate([low2,uplow8], axis = 3)\n",
        "    high8, low8 = OctaveConv2D(128)([merge8high, merge8low])\n",
        "    high8 = instance_norm()(high8)\n",
        "    high8 = layers.Activation(\"relu\")(high8)\n",
        "    low8 = instance_norm()(low8)\n",
        "    low8 = layers.Activation(\"relu\")(low8)\n",
        "    high8, low8 = OctaveConv2D(128)([high8, low8])\n",
        "    high8 = instance_norm()(high8)\n",
        "    high8 = layers.Activation(\"relu\")(high8)\n",
        "    low8 = instance_norm()(low8)\n",
        "    low8 = layers.Activation(\"relu\")(low8)\n",
        "\n",
        "    uphigh9, uplow9 = OctaveConv2D(64, use_transpose=True, strides=(2,2))([high8, low8])\n",
        "    uphigh9 = instance_norm()(uphigh9)\n",
        "    uphigh9 = layers.Activation(\"relu\")(uphigh9)\n",
        "    uplow9 = instance_norm()(uplow9)\n",
        "    uplow9 = layers.Activation(\"relu\")(uplow9)\n",
        "    merge9high = concatenate([high1,uphigh9], axis = 3)\n",
        "    merge9low = concatenate([low1,uplow9], axis = 3)\n",
        "    high9, low9 = OctaveConv2D(64)([merge9high, merge9low])\n",
        "    high9 = instance_norm()(high9)\n",
        "    high9 = layers.Activation(\"relu\")(high9)\n",
        "    low9 = instance_norm()(low9)\n",
        "    low9 = layers.Activation(\"relu\")(low9)\n",
        "    high9, low9 = OctaveConv2D(64)([high9, low9])\n",
        "    high9 = instance_norm()(high9)\n",
        "    high9 = layers.Activation(\"relu\")(high9)\n",
        "    low9 = instance_norm()(low9)\n",
        "    low9 = layers.Activation(\"relu\")(low9)\n",
        "    conv9 = OctaveConv2D(32, ratio_out=0.0)([high9, low9])\n",
        "    conv9 = layers.Activation(\"sigmoid\")(conv9)\n",
        "    conv10 = layers.Conv2D(3, 1, activation = 'tanh')(conv9)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=conv10)\n",
        "    \n",
        "    # model.summary()\n",
        "    \n",
        "    # model.compile(optimizer = RAdamOptimizer(learning_rate=1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    if(pretrained_weights):\n",
        "    \tmodel.load_weights(pretrained_weights)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ju9Wyw87MRW",
        "colab": {}
      },
      "source": [
        "from model import *\n",
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "generator_g = unet()\n",
        "generator_f = unet()\n",
        "\n",
        "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
        "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wDaGZ3WpZUyw",
        "colab": {}
      },
      "source": [
        "to_zebra = generator_g(sample_horse)\n",
        "to_horse = generator_f(sample_zebra)\n",
        "plt.figure(figsize=(8, 8))\n",
        "contrast = 8\n",
        "\n",
        "imgs = [sample_horse, to_zebra, sample_zebra, to_horse]\n",
        "title = ['Horse', 'To Zebra', 'Zebra', 'To Horse']\n",
        "\n",
        "for i in range(len(imgs)):\n",
        "  plt.subplot(2, 2, i+1)\n",
        "  plt.title(title[i])\n",
        "  if i % 2 == 0:\n",
        "    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n",
        "  else:\n",
        "    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O5MhJmxyZiy9",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Is a real zebra?')\n",
        "plt.imshow(discriminator_y(sample_zebra)[0, ..., -1], cmap='RdBu_r')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Is a real horse?')\n",
        "plt.imshow(discriminator_x(sample_horse)[0, ..., -1], cmap='RdBu_r')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JRqt02lupRn8"
      },
      "source": [
        "In CycleGAN, there is no paired data to train on, hence there is no guarantee that the input `x` and the target `y` pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the authors propose the cycle consistency loss.\n",
        "\n",
        "The discriminator loss and the generator loss are similar to the ones used in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#define_the_loss_functions_and_the_optimizer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cyhxTuvJyIHV",
        "colab": {}
      },
      "source": [
        "LAMBDA = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q1Xbz5OaLj5C",
        "colab": {}
      },
      "source": [
        "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wkMNfBWlT-PV",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real, generated):\n",
        "  real_loss = loss_obj(tf.ones_like(real), real)\n",
        "\n",
        "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss * 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90BIcCKcDMxz",
        "colab": {}
      },
      "source": [
        "def generator_loss(generated):\n",
        "  return loss_obj(tf.ones_like(generated), generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5iIWQzVF7f9e"
      },
      "source": [
        "Cycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the  original sentence.\n",
        "\n",
        "In cycle consistency loss, \n",
        "\n",
        "* Image $X$ is passed via generator $G$ that yields generated image $\\hat{Y}$.\n",
        "* Generated image $\\hat{Y}$ is passed via generator $F$ that yields cycled image $\\hat{X}$.\n",
        "* Mean absolute error is calculated between $X$ and $\\hat{X}$.\n",
        "\n",
        "$$forward\\ cycle\\ consistency\\ loss: X -> G(X) -> F(G(X)) \\sim \\hat{X}$$\n",
        "\n",
        "$$backward\\ cycle\\ consistency\\ loss: Y -> F(Y) -> G(F(Y)) \\sim \\hat{Y}$$\n",
        "\n",
        "\n",
        "![Cycle loss](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/cycle_loss.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NMpVGj_sW6Vo",
        "colab": {}
      },
      "source": [
        "def calc_cycle_loss(real_image, cycled_image):\n",
        "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "  \n",
        "  return LAMBDA * loss1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U-tJL-fX0Mq7"
      },
      "source": [
        "As shown above, generator $G$ is responsible for translating image $X$ to image $Y$. Identity loss says that, if you fed image $Y$ to generator $G$, it should yield the real image $Y$ or something close to image $Y$.\n",
        "\n",
        "$$Identity\\ loss = |G(Y) - Y| + |F(X) - X|$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "05ywEH680Aud",
        "colab": {}
      },
      "source": [
        "def identity_loss(real_image, same_image):\n",
        "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "  return LAMBDA * 0.5 * loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G-vjRM7IffTT"
      },
      "source": [
        "Initialize the optimizers for all the generators and the discriminators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iWCn_PVdEJZ7",
        "colab": {}
      },
      "source": [
        "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aKUZnDiqQrAh"
      },
      "source": [
        "## Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WJnftd5sQsv6",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
        "                           generator_f=generator_f,\n",
        "                           discriminator_x=discriminator_x,\n",
        "                           discriminator_y=discriminator_y,\n",
        "                           generator_g_optimizer=generator_g_optimizer,\n",
        "                           generator_f_optimizer=generator_f_optimizer,\n",
        "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
        "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Training\n",
        "\n",
        "Note: This example model is trained for fewer epochs (40) than the paper (200) to keep training time reasonable for this tutorial. Predictions may be less accurate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NS2GWywBbAWo",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RmdVsmvhPxyy",
        "colab": {}
      },
      "source": [
        "def generate_images(model, test_input):\n",
        "  prediction = model(test_input)\n",
        "    \n",
        "  plt.figure(figsize=(12, 12))\n",
        "\n",
        "  display_list = [test_input[0], prediction[0]]\n",
        "  title = ['Input Image', 'Predicted Image']\n",
        "\n",
        "  for i in range(2):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kE47ERn5fyLC"
      },
      "source": [
        "Even though the training loop looks complicated, it consists of four basic steps:\n",
        "\n",
        "* Get the predictions.\n",
        "* Calculate the loss.\n",
        "* Calculate the gradients using backpropagation.\n",
        "* Apply the gradients to the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KBKUV2sKXDbY",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(real_x, real_y):\n",
        "  # persistent is set to True because the tape is used more than\n",
        "  # once to calculate the gradients.\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    # Generator G translates X -> Y\n",
        "    # Generator F translates Y -> X.\n",
        "    \n",
        "    fake_y = generator_g(real_x, training=True)\n",
        "    cycled_x = generator_f(fake_y, training=True)\n",
        "\n",
        "    fake_x = generator_f(real_y, training=True)\n",
        "    cycled_y = generator_g(fake_x, training=True)\n",
        "\n",
        "    # same_x and same_y are used for identity loss.\n",
        "    same_x = generator_f(real_x, training=True)\n",
        "    same_y = generator_g(real_y, training=True)\n",
        "\n",
        "    disc_real_x = discriminator_x(real_x, training=True)\n",
        "    disc_real_y = discriminator_y(real_y, training=True)\n",
        "\n",
        "    disc_fake_x = discriminator_x(fake_x, training=True)\n",
        "    disc_fake_y = discriminator_y(fake_y, training=True)\n",
        "\n",
        "    # calculate the loss\n",
        "    gen_g_loss = generator_loss(disc_fake_y)\n",
        "    gen_f_loss = generator_loss(disc_fake_x)\n",
        "    \n",
        "    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
        "    \n",
        "    # Total generator loss = adversarial loss + cycle loss\n",
        "    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
        "    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
        "\n",
        "    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
        "    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
        "  \n",
        "  # Calculate the gradients for generator and discriminator\n",
        "  generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
        "                                        generator_g.trainable_variables)\n",
        "  generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
        "                                        generator_f.trainable_variables)\n",
        "  \n",
        "  discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
        "                                            discriminator_x.trainable_variables)\n",
        "  discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
        "                                            discriminator_y.trainable_variables)\n",
        "  \n",
        "  # Apply the gradients to the optimizer\n",
        "  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
        "                                            generator_g.trainable_variables))\n",
        "\n",
        "  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
        "                                            generator_f.trainable_variables))\n",
        "  \n",
        "  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
        "                                                discriminator_x.trainable_variables))\n",
        "  \n",
        "  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
        "                                                discriminator_y.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2M7LmLtGEMQJ",
        "colab": {}
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  n = 0\n",
        "  for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):\n",
        "    train_step(image_x, image_y)\n",
        "    if n % 10 == 0:\n",
        "      print ('.', end='')\n",
        "    n+=1\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  # Using a consistent image (sample_horse) so that the progress of the model\n",
        "  # is clearly visible.\n",
        "  generate_images(generator_g, sample_horse)\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "\n",
        "  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                      time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1RGysMU_BZhx"
      },
      "source": [
        "## Generate using test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUgSnmy2nqSP",
        "colab": {}
      },
      "source": [
        "# Run the trained model on the test dataset\n",
        "for inp in test_horses.take(5):\n",
        "  generate_images(generator_g, inp)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}